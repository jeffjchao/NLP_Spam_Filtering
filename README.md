# Text Classification – Spam vs. Ham
## Text Processing
In order to classify emails that are legitimate versus emails that are spam, the 
first step was to create a corpus that contained examples of both these types of 
messages. One good source of public emails is the Enron public email corpus, which 
was addended with a small number of additional spam emails to have a sufficient 
number of examples to train a classifier. The dataset used in this project had 3,672 
regular emails and 1,500 spam emails.

The first step towards processing the text was to import the corpus from the 
individual text files into python. This was done by incorporating the provided skeleton 
program called “classifySPAM.py” – this program iterates through each of the emails in
the relevant folders and puts all the words for each email in a list alongside a label for 
the type of message that it is. Additionally, the program also randomizes the list so that 
any possible ordering bias would be eliminated. Although 1,500 spam emails were 
available, I decided to use 1,000 emails from each category for a more even number.

In this text tokenization process, there are several points where further pre-processing 
or filtering can be done. However, for the initial classifier I wanted to create 
features based on all of the tokens present in the corpus, including the stop words and 
punctuation. In the experiments that followed, I did some additional processing to the 
tokens to remove some of these useless tokens to improve the accuracy of the classifier 
model.

## Feature Engineering
For the baseline test, the features were generated by simply using the 2,000
most common words in the corpus. However, by not filtering out any punctuation or stop 
words many of the top features in the documents seemed to be extremely low 
information and would not help the accuracy by much. 

After creating this list of word features to be checked, a function was written to 
help determine if each email contained each of the features in the set. This new function 
would label if each feature token was present in the corresponding email to give a list of 
2,000 true or false entries which would be used to determine (after training the 
classifier) if the email is a legitimate communication or spam.

From there, a Naïve Bayes classifier with 5-fold cross validation was trained on 
the corpus. The SciKit-Learn package in python was used to create the cross-validation 
folds from the labeled data so that the training and test sets could be constructed and 
verified against each other for each of the folds. Additionally, the precision and recall 
functions from the nltk.metrics package were imported to get a more dynamic view on 
evaluating how well each classifier performed; While accuracy gives us a good idea of 
the number of correct predictions from the entire list, precision gives a ratio that 
indicates the amount of false positives (emails labeled as spam but are not spam) while 
recall gives a ratio that indicates the amount of false negatives (emails not marked as 
spam but they actually are spam). Below are the values of these three metrics.

### Accuracy, Precision, and Recall for Baseline Naïve Bayes
|    | Accuracy | Precision | Recall |
|----|----|----|----|
| Baseline NB Classifier | 94.85% | 90.79% | 99.90% |

## Experiments
Additional features were also engineered for the additional experiments. The next 
features set that was created was for bigrams instead of unigrams used in the baseline. 
In order to clean up the bigram list, the stop words and punctuation were removed from
the bigram finder and the bigrams where sorted by the score of the raw frequencies for 
each pair of tokens. Running this new bigram feature set through the same 5-fold Naïve
Bayes classifier showed that accuracy, precision, and recall all suffered from using 
bigram analysis instead of unigrams. Therefore, further experiments would all use 
unigram feature sets to try to improve classifier accuracy.

After seeing no improvements from the features set using bigrams, the next 
engineered feature set was to do a similar cleaning for the unigrams but leaving some 
punctuation that appears more frequently in spam emails such as ‘!’ or ‘$’. Again, the 
stop word and remaining punctation tokens were removed from the feature list and the 
remaining tokens were used as word features for this second experiment. Running this 
new features set through the Naïve Bayes 5-fold cross validation showed improvements 
in accuracy and precision without any decrease in the recall.

Another recommended experiment for training on this corpus was to improve the 
feature vocabulary. In this case, I decided to improve the feature list to 5,000 words 
compared to the 2,000 words that were originally used in the baseline. In order to have 
an apples-to-apples comparison, I also decided against filtering out stop words and 
punctuation from the second experiment in order to see the effects from the single 
variable change in the metrics of the experiment. Training the Naïve Bayes classifier on 
this features set seemed to improve the accuracy, precision, and recall compared to the 
baseline; the improvements were roughly the same amount as when the features set 
was its original size but cleaned of stop words and punctuation.

Finally, since both cleaning the feature set and increasing the feature vocabulary 
both independently increased performance of the classifier, I decided to combine both 
these changes into a single classifier. For this new feature set, I first cleaned the corpus 
of the stop words and punctuation like in the second experiment and then constructed 
the features from the 5,000 most common tokens. Running this latest expanded 
features set through the Naïve Bayes 5-fold cross validation, it proved to provide the 
best scores in both accuracy and precision and the sacrifice of 0.1% decrease in recall.
The results from all 4 experiments have been provided below, as well as the top 50 
features from each of the feature sets.

### Metrics for Naïve Bayes Classifier Performance for each Feature Set
|    | Accuracy | Precision | Recall | Training Time |
|----|----|----|----|----|
| Baseline (Unigrams) | 94.85% | 90.79% | 99.90% | 29.18 sec |
| Bigram Features | 93.00% | 88.35% | 99.10% | 30.12 sec |
| Cleaned Unigrams | 96.15% | 92.97% | 99.90% | 29.85 sec |
| Expanded Unigrams | 96.25% | 93.09% | 100.00% | 76.23 sec |
| Clean+Expanded Unigrams | 97.60% | 95.61% | 99.80% | 77.58 sec |

## Conclusions
Based on the performance of sentiment analysis classifier in previous 
assignments, I originally expected poor or middling performance when using a Naïve
Bayes classifier to separate spam from ham emails. However, the results speak for 
themselves when the accuracy, prevision, and recall push 90% metrics even for the 
baseline classifier.

Based on these results, my recommendation for the feature set is best for 
classifying spam email is the combination of a cleaned unigram feature set with 
expanded feature vocabulary. Although the recall is 0.2% worse than some of the other 
experiments, that difference seems to be within statistical margin of error compared to 
the up to 2% improvements for the other categories. However, a clear trade-off that can 
be seen for increasing the feature vocabulary is an increase in training time for the 
classifier; while this amount of computational power is still very reasonable for this 
project, there may be instances where simply increasing vocabulary would be 
prohibitively expensive. In that case, a solid backup classifier for distinguishing spam 
from ham would be the cleaned unigrams features set.

For future forays in crafting classifiers for spam versus ham, it would be 
interesting to look at some other more advanced classification models available in other 
platforms. While Naïve Bayes proved to perform very well in this instance, using support 
vector machines, random forest, or even various types of neural networks has proved to 
improve classifier performance drastically compared to the relatively simple Naïve
Bayes model.

### Top 50 Features for Each Experiment
|  Rank  | 1. Baseline NB Classifier | 2. Bigrams               | 3. Cleaned Unigrams |
| ---- | -------------------------- | ------------------------- | -------------------- |
| 1    | \-                         | ('ect', 'cc')             | ect                  |
| 2    | .                          | ('ect', 'subject')        | hou                  |
| 3    | /                          | ('daren', 'j')            | Subject              |
| 4    | ,                          | ('j', 'farmer')           | !                    |
| 5    | :                          | ('ami', 'chokshi')        | 2000                 |
| 6    | the                        | ('please', 'let')         | enron                |
| 7    | to                         | ('would', 'like')         | 3                    |
| 8    | ect                        | ('xls', 'Subject')        | $                    |
| 9    | and                        | ('e', 'lloyd')            | please               |
| 10   | @                          | ('robert', 'e')           | \`\`                 |
| 11   | of                         | ('enron', 'cc')           | 1                    |
| 12   | a                          | ('b', 'camp')             | com                  |
| 13   | for                        | ('howard', 'b')           | 2                    |
| 14   | you                        | ('anita', 'luong')        | e                    |
| 15   | in                         | ('attached', 'file')      | %                    |
| 16   | is                         | ('north', 'america')      | 0                    |
| 17   | this                       | ('td', 'td')              | #                    |
| 18   | hou                        | ('see', 'attached')       | \>                   |
| 19   | on                         | ('pat', 'clynes')         | subject              |
| 20   | #ERROR!                    | ('brenda', 'f')           | meter                |
| 21   | ?                          | ('f', 'herod')            | pm                   |
| 22   | i                          | ('enron', 'north')        | gas                  |
| 23   |                            | ('looking', 'statements') | cc                   |
| 24   | )                          | ('jackie', 'young')       | deal                 |
| 25   | (                          | ('julie', 'meyers')       | 0                    |
| 26   | Subject                    | ('america', 'corp')       | http                 |
| 27   | !                          | ('aimee', 'lannou')       | 99                   |
| 28   | your                       | ('tr', 'td')              | 0                    |
| 29   | that                       | ('jpg', 'width')          | corp                 |
| 30   | be                         | ('cotton', 'valley')      | new                  |
| 31   | 2000                       | ('please', 'call')        | thanks               |
| 32   | enron                      | ('fuels', 'cotton')       | get                  |
| 33   | with                       | ('make', 'sure')          | 10                   |
| 34   | will                       | ('tom', 'acton')          | 4                    |
| 35   | we                         | ('xp', 'professional')    | 5                    |
| 36   | have                       | ('windows', 'xp')         | &                    |
| 37   | 3                          | ('sitara', 'deal')        | know                 |
| 38   | from                       | ('gas', 'marketing')      | |                    |
| 39   | $                          | ('fred', 'boas')          | need                 |
| 40   | s                          | ('stephanie', 'gomes')    | forwarded            |
| 41   | as                         | ('tr', 'tr')              | 3                    |
| 42   | at                         | ('email', 'address')      | 12                   |
| 43   | are                        | ('l', 'taylor')           | p                    |
| 44   | it                         | ('melissa', 'graves')     | may                  |
| 45   | by                         | ('vance', 'l')            | daren                |
| 46   | please                     | ('money', 'back')         | hpl                  |
| 47   | not                        | ('retail', 'price')       | 7                    |
| 48   | \`\`                       | ('charlotte', 'hawkins')  | information          |
| 49   | if                         | ('duty', 'free')          | us                   |
| 50   | 1                          | ('l', 'morris')           | company              |
